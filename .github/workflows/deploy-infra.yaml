name: deploy infra

on:
  push:
    branches:
      - main
      - develop
    paths:
      - "infra/cfn/cognito.yaml"
      - "infra/cfn/networking.yaml"
      - "infra/cfn/eks.yaml"
      - "infra/cfn/karpenter.yaml"
      - ".github/workflows/deploy-infra.yaml"
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

jobs:
  set-env:
    runs-on: ubuntu-latest
    outputs:
      env: ${{ steps.set-env.outputs.env }}
    steps:
      - name: Determine environment
        id: set-env
        run: |
          if [ "${{ github.ref }}" == "refs/heads/main" ]; then
            echo "env=production" >> $GITHUB_OUTPUT
          else
            echo "env=development" >> $GITHUB_OUTPUT
          fi

  build-image:
    uses: ./.github/workflows/build-images.yaml
    secrets: inherit
    
  deploy:
    needs: set-env
    runs-on: ubuntu-latest
    environment: ${{ needs.set-env.outputs.env }}

    env:
      AWS_REGION: ap-southeast-2
      APP_NAME: roommate
      NETWORKING_STACK_NAME: roommate-networking-${{ vars.ENV_SHORTHAND }}
      EKS_STACK_NAME: roommate-eks-${{ vars.ENV_SHORTHAND }}
      KARPENTER_STACK_NAME: roommate-karpenter-${{ vars.ENV_SHORTHAND }}
      COGNITO_STACK_NAME: roommate-cognito-${{ vars.ENV_SHORTHAND }}
      LB_CONTROLLER_VERSION: "2.9.0"

    steps:
      - name: deploy networking cfn
        run: echo ${{ needs.set-env.outputs.env }}

      - name: checkout
        uses: actions/checkout@v4

      - name: configure aws credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: login to docker hub
        uses: docker/login-action@v3
        with:
          username: grae888
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: install eksctl
        run: |
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz
          sudo mv eksctl /usr/local/bin

      - name: deploy networking cfn
        run: |
          aws cloudformation deploy \
            --stack-name "${NETWORKING_STACK_NAME}" \
            --template-file infra/cfn/networking.yaml \
            --region "${AWS_REGION}" \
            --no-fail-on-empty-changeset \
            --parameter-overrides \
              Env="${{ vars.ENV_SHORTHAND }}" \
              VpcCidr="${{ vars.VPC_CIDR }}"

      - name: deploy eks cfn
        run: |
          aws cloudformation deploy \
            --stack-name "${EKS_STACK_NAME}" \
            --template-file infra/cfn/eks.yaml \
            --region "${AWS_REGION}" \
            --no-fail-on-empty-changeset \
            --capabilities CAPABILITY_NAMED_IAM \
            --parameter-overrides \
            Env="${{ vars.ENV_SHORTHAND }}" \
            NetworkStackName="${NETWORKING_STACK_NAME}" \
            NodeInstanceType="${{ vars.NODE_INSTANCE_TYPE }}" \
            NodeDesiredSize="${{ vars.NODE_DESIRED_SIZE }}" \
            NodeMinSize="${{ vars.NODE_MIN_SIZE }}" \
            NodeMaxSize="${{ vars.NODE_MAX_SIZE }}" \
            DomainName="${{ vars.DOMAIN_NAME }}" \
            HostedZoneId="${{ secrets.HOSTED_ZONE_ID }}"
      
      - name: deploy karpenter cfn
        run: |
          aws cloudformation deploy \
            --stack-name "${KARPENTER_STACK_NAME}" \
            --template-file infra/cfn/karpenter.yaml \
            --region "${AWS_REGION}" \
            --no-fail-on-empty-changeset \
            --capabilities CAPABILITY_NAMED_IAM \
            --parameter-overrides \
            Env="${{ vars.ENV_SHORTHAND }}" \
            NetworkStackName="${NETWORKING_STACK_NAME}" \
            EksStackName="${EKS_STACK_NAME}"

      - name: deploy cognito cfn
        run: |
          aws cloudformation deploy \
            --stack-name "${COGNITO_STACK_NAME}" \
            --template-file infra/cfn/cognito.yaml \
            --region "${AWS_REGION}" \
            --no-fail-on-empty-changeset \
            --parameter-overrides \
            Env="${{ vars.ENV_SHORTHAND }}" \
            DomainName="${{ vars.DOMAIN_NAME }}" \
            GoogleClientId="${{ secrets.GOOGLE_CLIENT_ID }}" \
            GoogleClientSecret="${{ secrets.GOOGLE_CLIENT_SECRET }}"
      
      - name: get helm inputs
        run: |
          echo "getting cluster name..."
          CLUSTER_NAME=$(aws cloudformation describe-stacks \
          --stack-name ${EKS_STACK_NAME} \
          --query "Stacks[0].Outputs[?OutputKey=='ClusterName'].OutputValue" \
          --output text)
          echo "cluster name: '$CLUSTER_NAME'"
          echo "CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV

          echo "getting karpenter controller role arn..."
          CONTROLLER_ROLE_ARN=$(aws cloudformation describe-stacks \
          --stack-name ${KARPENTER_STACK_NAME} \
          --query "Stacks[0].Outputs[?OutputKey=='KarpenterControllerRoleArn'].OutputValue" \
          --output text)
          echo "karpenter controller role arn: '$CONTROLLER_ROLE_ARN'"
          echo "CONTROLLER_ROLE_ARN=$CONTROLLER_ROLE_ARN" >> $GITHUB_ENV

          echo "getting node role arn..."
          NODE_ROLE=$(aws cloudformation describe-stacks \
          --stack-name ${KARPENTER_STACK_NAME} \
          --query "Stacks[0].Outputs[?OutputKey=='KarpenterNodeRoleArn'].OutputValue" \
          --output text)
          echo "node role arn: '$NODE_ROLE'"
          echo "NODE_ROLE=$NODE_ROLE" >> $GITHUB_ENV

          echo "getting node role name..."
          NODE_ROLE_NAME="${NODE_ROLE##*/}"
          echo "node role name: '$NODE_ROLE_NAME'"
          echo "NODE_ROLE_NAME=$NODE_ROLE_NAME" >> $GITHUB_ENV

          echo "getting vpc id..."
          VPC_ID=$(aws cloudformation describe-stacks \
          --stack-name ${NETWORKING_STACK_NAME} \
          --query "Stacks[0].Outputs[?OutputKey=='VpcId'].OutputValue" \
          --output text)
          echo "vpc id: '$VPC_ID'"
          echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV

          echo "getting ingress certificate arn..."
          CERTIFICATE_ARN=$(aws cloudformation describe-stacks \
          --stack-name ${EKS_STACK_NAME} \
          --query "Stacks[0].Outputs[?OutputKey=='CertificateArn'].OutputValue" \
          --output text)
          echo "ingress certificate arn: '$CERTIFICATE_ARN'"
          echo "CERTIFICATE_ARN=$CERTIFICATE_ARN" >> $GITHUB_ENV

          echo "getting cognito user pool arn..."
          USER_POOL_ARN=$(aws cloudformation describe-stacks \
          --stack-name ${COGNITO_STACK_NAME} \
          --query "Stacks[0].Outputs[?OutputKey=='UserPoolArn'].OutputValue" \
          --output text)
          echo "user pool arn: '$USER_POOL_ARN'"
          echo "USER_POOL_ARN=$USER_POOL_ARN" >> $GITHUB_ENV

          echo "getting cognito user pool client id..."
          USER_POOL_CLIENT_ID=$(aws cloudformation describe-stacks \
          --stack-name ${COGNITO_STACK_NAME} \
          --query "Stacks[0].Outputs[?OutputKey=='UserPoolClientId'].OutputValue" \
          --output text)
          echo "user pool client id: '$USER_POOL_CLIENT_ID'"
          echo "USER_POOL_CLIENT_ID=$USER_POOL_CLIENT_ID" >> $GITHUB_ENV

          echo "getting cognito user pool domain prefix..."
          USER_POOL_DOMAIN_PREFIX=$(aws cloudformation describe-stacks \
          --stack-name ${COGNITO_STACK_NAME} \
          --query "Stacks[0].Outputs[?OutputKey=='UserPoolDomainPrefix'].OutputValue" \
          --output text)
          echo "user pool domain prefix: '$USER_POOL_DOMAIN_PREFIX'"
          echo "USER_POOL_DOMAIN_PREFIX=$USER_POOL_DOMAIN_PREFIX" >> $GITHUB_ENV

      - name: deploy karpenter controller
        run: |
          echo "configuring kubectl connection to cluster..."
          aws eks update-kubeconfig --name $CLUSTER_NAME --region ${AWS_REGION}

          echo "executing karpenter controller helm..."
          helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
            --version "1.0.0" \
            --namespace karpenter \
            --create-namespace \
            --set "settings.clusterName=$CLUSTER_NAME" \
            --set "settings.interruptionQueue=$CLUSTER_NAME" \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"=$CONTROLLER_ROLE_ARN \
            --set replicas=${{ vars.NODE_DESIRED_SIZE }}
      
      - name: deploy karpenter nodepool
        run: |
          echo "creating node role identity mapping..."
          eksctl create iamidentitymapping \
            --cluster "$CLUSTER_NAME" \
            --arn "$NODE_ROLE" \
            --username "system:node:{{EC2PrivateDNSName}}" \
            --group "system:bootstrappers" \
            --group "system:nodes" \
            --region "${{ env.AWS_REGION }}"

          echo "executing karpenter nodepool helm..."
          helm upgrade --install karpenter-nodepool ./infra/helm/karpenter \
            -f ./infra/helm/karpenter/values.${{ vars.ENV_SHORTHAND }}.yaml \
            --set appName=${APP_NAME}-${{ vars.ENV_SHORTHAND }} \
            --set nodeRole=$NODE_ROLE_NAME \
            --namespace karpenter

      - name: eks load balancer controller iam policy
        id: iam-policy
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          POLICY_NAME="AWSLoadBalancerControllerIAMPolicy"
          POLICY_ARN="arn:aws:iam::${ACCOUNT_ID}:policy/${POLICY_NAME}"
          
          if aws iam get-policy --policy-arn "$POLICY_ARN" 2>/dev/null; then
            echo "IAM Policy already exists"
          else
            echo "Creating IAM Policy..."
            curl -fsSL -o iam_policy.json \
              https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v${{ env.LB_CONTROLLER_VERSION }}/docs/install/iam_policy.json
            aws iam create-policy --policy-name "$POLICY_NAME" --policy-document file://iam_policy.json
          fi
          
          echo "POLICY_ARN=$POLICY_ARN" >> $GITHUB_ENV
      
      - name: eks load balancer controller service account + role
        run: |
          eksctl create iamserviceaccount \
            --cluster=$CLUSTER_NAME \
            --region=${{ env.AWS_REGION }} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn=$POLICY_ARN \
            --override-existing-serviceaccounts \
            --approve
      
      - name: eks load balancer controller helm chart
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --namespace kube-system \
            --set clusterName=$CLUSTER_NAME \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set region=${{ env.AWS_REGION }} \
            --set vpcId=$VPC_ID \
            --wait

      - name: deploy app infra
        run: |
          echo "executing app helm..."
          helm upgrade --install app ./infra/helm/app \
            -f ./infra/helm/app/values.${{ vars.ENV_SHORTHAND }}.yaml \
            --set frontend.ingress.certificateArn=$CERTIFICATE_ARN \
            --set frontend.ingress.cognito.userPoolArn=$USER_POOL_ARN \
            --set frontend.ingress.cognito.userPoolClientId=$USER_POOL_CLIENT_ID \
            --set frontend.ingress.cognito.userPoolDomainPrefix=$USER_POOL_DOMAIN_PREFIX \

      - name: get ingress alb hostname
        run: |
          INGRESS_NAME=${{ env.APP_NAME }}-ingress

          # wait for alb to be provisioned (up to 5 minutes)
          for i in {1..30}; do
            ALB_HOSTNAME=$(kubectl get ingress $INGRESS_NAME \
              -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
            
            if [ -n "${ALB_HOSTNAME}" ]; then
              echo "alb hostname: ${ALB_HOSTNAME}"
              echo "ALB_HOSTNAME=${ALB_HOSTNAME}" >> $GITHUB_ENV
              break
            fi
            
            echo "waiting for alb to be provisioned... ($i/30)"
            sleep 10
          done

          if [ -z "${ALB_HOSTNAME}" ]; then
            echo "alb not found"
            exit 1
          fi

      - name: get ingress alb hosted zone
        run: |
          ALB_HOSTED_ZONE_ID=$(aws elbv2 describe-load-balancers \
            --region ${{ env.AWS_REGION }} \
            --query "LoadBalancers[?DNSName=='${ALB_HOSTNAME}'].CanonicalHostedZoneId" \
            --output text)

          echo "got hosted zone id"
          echo "ALB_HOSTED_ZONE_ID=${ALB_HOSTED_ZONE_ID}" >> $GITHUB_ENV

      - name: create ingress alb record
        run: |
          cat > change_batch.json << EOF
          {
            "Changes": [
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "${{ vars.DOMAIN_NAME }}",
                  "Type": "A",
                  "AliasTarget": {
                    "HostedZoneId": "${ALB_HOSTED_ZONE_ID}",
                    "DNSName": "${ALB_HOSTNAME}",
                    "EvaluateTargetHealth": true
                  }
                }
              }
            ]
          }
          EOF

          echo "creating dns record..."
          cat change_batch.json

          aws route53 change-resource-record-sets \
            --hosted-zone-id ${{ secrets.HOSTED_ZONE_ID }} \
            --change-batch file://change_batch.json

          echo "dns record created ${{ vars.DOMAIN_NAME }} -> ${ALB_HOSTNAME}"