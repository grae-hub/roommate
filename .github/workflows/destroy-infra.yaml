name: destroy infra

on:
  schedule:
    - cron: "0 20 * * *"
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

jobs:
  cleanup:
    runs-on: ubuntu-latest
    strategy:
        matrix:
          environment: [development, production]
    
    environment: ${{ matrix.environment }}

    env:
        AWS_REGION: ap-southeast-2
        EKS_STACK_NAME: roommate-eks-${{ vars.ENV_SHORTHAND }}
        STACKS_TO_DELETE: |
            roommate-cognito-${{ vars.ENV_SHORTHAND }}
            roommate-karpenter-${{ vars.ENV_SHORTHAND }}
            roommate-eks-${{ vars.ENV_SHORTHAND }}
            roommate-networking-${{ vars.ENV_SHORTHAND }}
        KARPENTER_ROLE_NAME: KarpenterNodeRole-roommate-eks-cluster-${{ vars.ENV_SHORTHAND }}
    
    steps:
      - name: configure aws credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: configure kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.32.0'
      
      - name: configure kubeconfig for eks
        id: configure-kubeconfig
        run: |
          echo "checking eks stack exists..."
          STACK_STATUS=$(aws cloudformation describe-stacks \
            --stack-name "${EKS_STACK_NAME}" \
            --query 'Stacks[0].StackStatus' \
            --output text 2>/dev/null || echo "DOES_NOT_EXIST")

          if [[ "$STACK_STATUS" == "DOES_NOT_EXIST" || "$STACK_STATUS" == "DELETE_COMPLETE" ]]; then
            echo "stack '${EKS_STACK_NAME}' does not exist or is deleted"
            echo "skip=true" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          
          echo "skip=false" >> "$GITHUB_OUTPUT"
          
          echo "getting cluster name..."
          CLUSTER_NAME=$(aws cloudformation describe-stacks \
            --stack-name "${EKS_STACK_NAME}" \
            --query 'Stacks[0].Outputs[?OutputKey==`ClusterName`].OutputValue' \
            --output text)
          
          echo "cluster name: '$CLUSTER_NAME'"
          echo "CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
          
          aws eks update-kubeconfig \
            --name "$CLUSTER_NAME" \
            --region "${{ env.AWS_REGION }}"

      - name: scale deployments to zero
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          IGNORE_NS_REGEX='^(kube-system|kube-public|kube-node-lease|karpenter|amazon-cloudwatch)$'

          # loop over namespaces
          for ns in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep -Ev "${IGNORE_NS_REGEX}"); do
            echo "Processing namespace: ${ns}"

            # loop over deployments in namespace
            deploys=$(kubectl get deploy -n "${ns}" --no-headers 2>/dev/null | awk '{print $1}')

            if [ -z "${deploys}" ]; then
              echo "no deployments in ${ns}"
              continue
            fi

            for d in $deploys; do
              echo "scaling deployment/${d} in ${ns} to 0"
              kubectl scale deployment "${d}" -n "${ns}" --replicas=0
            done
          done

      - name: wait for pods to scale
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          IGNORE_NS_REGEX='^[[:space:]]*(kube-system|kube-public|kube-node-lease|karpenter|amazon-cloudwatch)[[:space:]]'

          timeout_seconds=600
          end=$((SECONDS + timeout_seconds))

          while [ $SECONDS -lt $end ]; do
            remaining_pods=$(kubectl get pods -A \
              --field-selector=status.phase!=Succeeded,status.phase!=Failed \
              --no-headers 2>/dev/null | \
              grep -Ev "$IGNORE_NS_REGEX" || true)

            if [ -n "$remaining_pods" ]; then
              echo "remaining pods:"
              echo "$remaining_pods"
            fi

            if [ -z "$remaining_pods" ]; then
              remaining=0
            else
              remaining=$(printf '%s\n' "$remaining_pods" | wc -l)
            fi

            if [ "${remaining}" -eq 0 ]; then
              echo "all pods terminated"
              exit 0
            fi

            echo "${remaining} non-system pods still running..."
            sleep 10
          done

          echo "timed out"
          exit 1

      - name: install eksctl
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz
          sudo mv eksctl /usr/local/bin

      - name: uninstall load balancer service account
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          SERVICE_ACCOUNT=aws-load-balancer-controller
          NS=kube-system
          
          if eksctl get iamserviceaccount \
            --cluster "$CLUSTER_NAME" \
            --region "${{ env.AWS_REGION }}" \
            -o json | jq -e ".[] | select(.metadata.name==\"$SERVICE_ACCOUNT\" and .metadata.namespace==\"$NS\")" >/dev/null; then
        
            echo "service account '$SERVICE_ACCOUNT' exists, deleting..."
        
            eksctl delete iamserviceaccount \
              --cluster "$CLUSTER_NAME" \
              --region "${{ env.AWS_REGION }}" \
              --namespace "$NS" \
              --name "$SERVICE_ACCOUNT"
          
          else
              echo "service account '$SERVICE_ACCOUNT' does not exist"
          fi

      - name: uninstall all helms
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          HELM_RELEASES=(
            "app default"
            "karpenter-nodepool karpenter"
            "karpenter karpenter"
            "aws-load-balancer-controller kube-system"
          )
      
          for entry in "${HELM_RELEASES[@]}"; do
            read -r RELEASE NS <<< "$entry"
            if helm status "$RELEASE" -n "$NS" >/dev/null 2>&1; then
              echo "uninstalling helm '$RELEASE'..."
              if ! helm uninstall "$RELEASE" -n "$NS" --wait --timeout 5m0s; then
                echo "failed to uninstall helm '$RELEASE' in '$NS'"
                continue
              fi
            else
              echo "helm '$RELEASE' does not exist"
            fi
          done

      - name: delete all ingresses
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          IGNORE_NS_REGEX='^(kube-system|kube-public|kube-node-lease|karpenter|amazon-cloudwatch)$'
      
          # loop over namespaces
          NAMESPACES=$(kubectl get ns -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep -Ev "$IGNORE_NS_REGEX" || true)
          for ns in $NAMESPACES; do
            echo "processing namespace $ns"
      
            ingresses=$(kubectl get ingress -n "$ns" --no-headers 2>/dev/null | awk '{print $1}' || true)
      
            if [ -z "$ingresses" ]; then
              echo "no ingresses in ${ns}"
              continue
            fi
      
            for i in $ingresses; do
              echo "deleting ingress/${i} in ${ns}"
              if ! kubectl delete ingress "${i}" -n "${ns}" --wait=true --timeout=120s --ignore-not-found=true; then
                echo "failed to delete ingress ${i} in ${ns} (timeout or error)"
                continue
              fi
            done
          done

      - name: delete all services
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          IGNORE_NS_REGEX='^(kube-system|kube-public|kube-node-lease|karpenter|amazon-cloudwatch)$'
      
          # loop over namespaces
          NAMESPACES=$(kubectl get ns -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep -Ev "$IGNORE_NS_REGEX" || true)
          for ns in $NAMESPACES; do
            echo "Processing namespace: ${ns}"
      
            # list services excluding the default Kubernetes service
            services=$(kubectl get svc -n "${ns}" --no-headers 2>/dev/null | awk '{print $1}' | grep -v '^kubernetes$' || true)
      
            if [ -z "${services}" ]; then
              echo "no services in ${ns}"
              continue
            fi
      
            for i in $services; do
              echo "deleting service/${i} in ${ns}"
              if ! kubectl delete service "${i}" -n "${ns}" --wait=true --timeout=120s --ignore-not-found=true; then
                echo "failed to delete service ${i} in ${ns} (timeout or error)"
                continue
              fi
            done
          done
      
      - name: delete all deployments
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          IGNORE_NS_REGEX='^(kube-system|kube-public|kube-node-lease|karpenter|amazon-cloudwatch)$'
      
          # loop over namespaces
          NAMESPACES=$(kubectl get ns -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep -Ev "$IGNORE_NS_REGEX" || true)
          for ns in $NAMESPACES; do
            echo "Processing namespace: ${ns}"
      
            # list deployments
            deployments=$(kubectl get deployments -n "${ns}" --no-headers 2>/dev/null | awk '{print $1}' || true)
      
            if [ -z "${deployments}" ]; then
              echo "no deployments in ${ns}"
              continue
            fi
      
            for i in $deployment; do
              echo "deleting deployment/${i} in ${ns}"
              if ! kubectl delete deployment "${i}" -n "${ns}" --wait=true --timeout=120s --ignore-not-found=true; then
                echo "failed to delete deployment ${i} in ${ns} (timeout or error)"
                continue
              fi
            done
          done

      - name: delete load balancer security groups
        run: |
          aws ec2 describe-security-groups \
            --query "SecurityGroups[?contains(GroupName, 'roommate')].GroupId" \
            --output text | tr '\t' '\n' | while read sg_id; do
              if [ -n "$sg_id" ]; then
                echo "deleting security group $sg_id"
                aws ec2 delete-security-group --group-id "$sg_id" || echo "Failed to delete $sg_id"
              fi
            done

      - name: delete stacks if they exist
        run: |
          for stack in $STACKS_TO_DELETE; do
            echo "checking stack '$stack' exists"
            
            if aws cloudformation describe-stacks --stack-name "$stack" >/dev/null 2>&1; then
              echo "deleting stack '$stack'..."
              aws cloudformation delete-stack --stack-name "$stack"
              aws cloudformation wait stack-delete-complete --stack-name "$stack"
              echo "stack '$stack' deleted"
            else
              echo "stack '$stack' does not exist, skipping"
            fi
          done