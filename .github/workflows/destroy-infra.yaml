name: destroy infra

on:
  schedule:
    - cron: "0 20 * * *"
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

jobs:
  cleanup:
    runs-on: ubuntu-latest
    strategy:
        matrix:
          environment: [development, production]
    
    environment: ${{ matrix.environment }}

    env:
        AWS_REGION: ap-southeast-2
        EKS_STACK_NAME: roommate-eks-${{ vars.ENV_SHORTHAND }}
        STACKS_TO_DELETE: |
            roommate-cognito-${{ vars.ENV_SHORTHAND }}
            roommate-karpenter-${{ vars.ENV_SHORTHAND }}
            roommate-eks-${{ vars.ENV_SHORTHAND }}
            roommate-networking-${{ vars.ENV_SHORTHAND }}
        KARPENTER_ROLE_NAME: KarpenterNodeRole-roommate-eks-cluster-${{ vars.ENV_SHORTHAND }}
        CLUSTER_NAME: roommate-eks-cluster-${{ vars.ENV_SHORTHAND }}
    
    steps:
      - name: configure aws credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: configure kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.32.0'
      
      - name: configure kubeconfig for eks
        id: configure-kubeconfig
        run: |
          echo "checking eks stack exists..."
          STACK_STATUS=$(aws cloudformation describe-stacks \
            --stack-name "${EKS_STACK_NAME}" \
            --query 'Stacks[0].StackStatus' \
            --output text 2>/dev/null || echo "DOES_NOT_EXIST")

          if [[ "$STACK_STATUS" == "DOES_NOT_EXIST" || "$STACK_STATUS" == "DELETE_COMPLETE" ]]; then
            echo "stack '${EKS_STACK_NAME}' does not exist or is deleted"
            echo "skip=true" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          
          echo "skip=false" >> "$GITHUB_OUTPUT"
          
          aws eks update-kubeconfig \
            --name "${{ env.CLUSTER_NAME }}" \
            --region "${{ env.AWS_REGION }}"

      - name: test kubeconfig
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          kubectl get ingresses

      - name: install eksctl
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz"
          tar -xzf eksctl_Linux_amd64.tar.gz
          sudo mv eksctl /usr/local/bin

      - name: uninstall load balancer service account
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          SERVICE_ACCOUNT=aws-load-balancer-controller
          NS=kube-system
          
          if eksctl get iamserviceaccount \
            --cluster "${{ env.CLUSTER_NAME }}" \
            --region "${{ env.AWS_REGION }}" \
            -o json | jq -e ".[] | select(.metadata.name==\"$SERVICE_ACCOUNT\" and .metadata.namespace==\"$NS\")" >/dev/null; then
        
            echo "service account '$SERVICE_ACCOUNT' exists, deleting..."
        
            eksctl delete iamserviceaccount \
              --cluster "${{ env.CLUSTER_NAME }}" \
              --region "${{ env.AWS_REGION }}" \
              --namespace "$NS" \
              --name "$SERVICE_ACCOUNT"
          
          else
              echo "service account '$SERVICE_ACCOUNT' does not exist"
          fi

      - name: uninstall all helms
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          HELM_RELEASES=(
            "app default"
            "karpenter-nodepool karpenter"
            "karpenter karpenter"
            "aws-load-balancer-controller kube-system"
          )
      
          for entry in "${HELM_RELEASES[@]}"; do
            read -r RELEASE NS <<< "$entry"
            if helm status "$RELEASE" -n "$NS" >/dev/null 2>&1; then
              echo "uninstalling helm '$RELEASE'..."
              if ! helm uninstall "$RELEASE" -n "$NS" --wait --debug --timeout 5m0s; then
                echo "failed to uninstall helm '$RELEASE' in '$NS'"
                continue
              fi
            else
              echo "helm '$RELEASE' does not exist"
            fi
          done

      - name: delete all ingresses
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          IGNORE_NS_REGEX='^(kube-system|kube-public|kube-node-lease|karpenter|amazon-cloudwatch)$'
      
          # loop over namespaces
          NAMESPACES=$(kubectl get ns -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep -Ev "$IGNORE_NS_REGEX" || true)
          for ns in $NAMESPACES; do
            echo "processing namespace $ns"
      
            ingresses=$(kubectl get ingress -n "$ns" --no-headers 2>/dev/null | awk '{print $1}' || true)
      
            if [ -z "$ingresses" ]; then
              echo "no ingresses in ${ns}"
              continue
            fi
      
            for i in $ingresses; do
              echo "deleting ingress/${i} in ${ns}"
              if ! kubectl delete ingress "${i}" -n "${ns}" --wait=true --timeout=120s --ignore-not-found=true; then
                echo "failed to delete ingress ${i} in ${ns} (timeout or error)"
                continue
              fi
            done
          done

      - name: delete all services
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          IGNORE_NS_REGEX='^(kube-system|kube-public|kube-node-lease|karpenter|amazon-cloudwatch)$'
      
          # loop over namespaces
          NAMESPACES=$(kubectl get ns -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep -Ev "$IGNORE_NS_REGEX" || true)
          for ns in $NAMESPACES; do
            echo "Processing namespace: ${ns}"
      
            # list services excluding the default Kubernetes service
            services=$(kubectl get svc -n "${ns}" --no-headers 2>/dev/null | awk '{print $1}' | grep -v '^kubernetes$' || true)
      
            if [ -z "${services}" ]; then
              echo "no services in ${ns}"
              continue
            fi
      
            for i in $services; do
              echo "deleting service/${i} in ${ns}"
              if ! kubectl delete service "${i}" -n "${ns}" --wait=true --timeout=120s --ignore-not-found=true; then
                echo "failed to delete service ${i} in ${ns} (timeout or error)"
                continue
              fi
            done
          done
      
      - name: delete all deployments
        if: steps.configure-kubeconfig.outputs.skip != 'true'
        run: |
          IGNORE_NS_REGEX='^(kube-system|kube-public|kube-node-lease|karpenter|amazon-cloudwatch)$'
      
          # loop over namespaces
          NAMESPACES=$(kubectl get ns -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep -Ev "$IGNORE_NS_REGEX" || true)
          for ns in $NAMESPACES; do
            echo "Processing namespace: ${ns}"
      
            # list deployments
            deployments=$(kubectl get deployments -n "${ns}" --no-headers 2>/dev/null | awk '{print $1}' || true)
      
            if [ -z "${deployments}" ]; then
              echo "no deployments in ${ns}"
              continue
            fi
      
            for i in $deployment; do
              echo "deleting deployment/${i} in ${ns}"
              if ! kubectl delete deployment "${i}" -n "${ns}" --wait=true --timeout=120s --ignore-not-found=true; then
                echo "failed to delete deployment ${i} in ${ns} (timeout or error)"
                continue
              fi
            done
          done

      - name: delete load balancer security groups
        run: |
          aws ec2 describe-security-groups \
            --query "SecurityGroups[?contains(GroupName, 'roommate')].GroupId" \
            --output text | tr '\t' '\n' | while read sg_id; do
              if [ -n "$sg_id" ]; then
                echo "deleting security group $sg_id"
                aws ec2 delete-security-group --group-id "$sg_id" || echo "Failed to delete $sg_id"
              fi
            done

      - name: delete stacks if they exist
        run: |
          for stack in $STACKS_TO_DELETE; do
            echo "checking stack '$stack' exists"
            
            if aws cloudformation describe-stacks --stack-name "$stack" >/dev/null 2>&1; then
              echo "deleting stack '$stack'..."
              aws cloudformation delete-stack --stack-name "$stack"
              aws cloudformation wait stack-delete-complete --stack-name "$stack"
              echo "stack '$stack' deleted"
            else
              echo "stack '$stack' does not exist, skipping"
            fi
          done